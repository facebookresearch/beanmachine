{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Neal's funnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates modeling and running inference on the so-called Neal's funnel model in Bean Machine.\n",
    "\n",
    "Neal's funnel has proven difficult-to-handle for classical inference methods. This tutorial demonstrates how to overcome this by using second-order gradient methods in Bean Machine. It also demonstrates how to implement models with factors in Bean Machine through custom distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Neal's funnel is a synthetic model that is fairly simple, but has proven challenging for automatic inference engines to handle due to its unusual geometry. This model has an unfavorable, exponential geometry in one direction, and a narrow \"funnel\" bending into that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Let's model this in Bean Machine! Import the Bean Machine library and some fundamental PyTorch classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beanmachine.ppl as bm\n",
    "from beanmachine.ppl.legacy.inference.proposer.single_site_hamiltonian_monte_carlo_proposer import (\n",
    "    SingleSiteHamiltonianMonteCarloProposer,\n",
    ")\n",
    "from beanmachine.ppl.legacy.inference.proposer.single_site_newtonian_monte_carlo_proposer import (\n",
    "    SingleSiteNewtonianMonteCarloProposer,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.distributions as dist\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "plt.rc('figure', figsize=[8, 6])\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('lines', linewidth=2.5)\n",
    "\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "torch.manual_seed(11);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "[Neal's funnel](https://projecteuclid.org/euclid.aos/1056562461) is defined mathematically as follows:\n",
    "\n",
    "* $z \\sim \\mathcal{N}(0, 3)$\n",
    "* $x \\sim \\mathcal{N}(0, e^{z/2})$\n",
    "\n",
    "Let's visualize the model's density. To do this, we recognize that the joint density is factored as follows:\n",
    "\n",
    "* $P(z, x) = \\mathcal{N}(x; 0, e^{z/2}) \\cdot \\mathcal{N}(z; 0, 3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, zs = torch.meshgrid(\n",
    "    torch.arange(-50, 50, 0.1),\n",
    "    torch.arange(-15.0, 15.0, 0.1),\n",
    ")\n",
    "density = (\n",
    "    dist.Normal(0.0, (zs / 2.0).exp()).log_prob(xs).exp() *\n",
    "    dist.Normal(0.0, 3.0).log_prob(zs).exp()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(xs, zs, density, levels=[0.0001] + torch.linspace(0.001, 0.1, 10).tolist())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the log density is usually easier to visualize and reason about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(xs, zs, density.log(), levels=range(-10, 0))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the funnelâ€™s neck is particularly sharp because of the exponential function applied to $z$. The density decays exponentially the farther that $x$ deviates from $0$. This makes it challenging to learn a good scale for proposal updates. Let's go about modeling this in Bean Machine!\n",
    "\n",
    "Compared to many of the other tutorials, this one is more of a contrived model. In the typical problem setup found in other tutorials, there are ground-truth values that we're trying to infer distributions about based on observed data. In this case, however, we're trying to exactly replicate a ground-truth _distribution_, by using the mechanics of the Bean Machine inference engine to guide the sampling process.\n",
    "\n",
    "Since Neal's funnel describes a mathematical relationship instead of a generative process, we'll have to reframe it into a generative process in order to run inference on it. We'll do this as follows:\n",
    "\n",
    "  1. Sample priors for $z$ and $x$.\n",
    "  2. Imagine weighting the probabilities of $z$ and $x$ according to how likely they are under the true Neal's funnel model. We can do this by imagining we're flipping a coin, where the probability of it landing heads is the probability of drawing that $z$ and $x$ from the Neal's funnel model, but where we've actually _observed_ it to be heads.\n",
    "  3. Later, we will inform the inference engine that we observed heads. This will cause the engine to find values for $z$ and $x$ that are consistent with samples from the true Neal's funnel posterior -- since those are the samples that would have resulted in the observed \"heads\" from our coin flip!\n",
    "  \n",
    "<div style=\"background: #daeaf3; border-left: 3px solid #2980b9; display: block; margin: 16px 0; padding: 12px;\">\n",
    "  A few notes for advanced readers (feel free to skip over these):\n",
    "  <ul>\n",
    "    <li>In the above statistical model, we already _had_ definitions for $z \\sim \\mathcal{N}(0, 3)$ and $x \\sim \\mathcal{N}(0, e^{z/2})$. We are free to reuse these definitions as priors. However, that's giving inference an unfair advantage, since our priors exactly match our posterior. Instead, in this tutorial we will choose noninformative priors.\n",
    "    <li>It is common to refer to this coin-flipping approach as a \"factor\". Traditionally, PPLs have been implemented by weighting a particular run of inference according to the log probability of that run of inference. We're exactly doing that in this model -- based on a particular draw of $z$ and $x$, we're weighting that overall run by the probability that those values would have been sampled from the true Neal's funnel posterior.\n",
    "  </ul>\n",
    "</div>\n",
    "  \n",
    "We can implement this model in Bean Machine by defining random variable objects with the `@bm.random_variable` decorator. These functions behave differently than ordinary Python functions.\n",
    "\n",
    "<div style=\"background: #daeaf3; border-left: 3px solid #2980b9; display: block; margin: 16px 0; padding: 12px;\">\n",
    "  Semantics for <code>@bm.random_variable</code> functions:\n",
    "  <ul>\n",
    "    <li>They must return PyTorch <code>Distribution</code> objects.\n",
    "    <li>Though they return distributions, callees actually receive <i>samples</i> from the distribution. The machinery for obtaining samples from distributions is handled internally by Bean Machine.\n",
    "    <li>Inference runs the model through many iterations. During a particular inference iteration, a distinct random variable will correspond to exactly one sampled value: <b>calls to the same random variable function with the same arguments will receive the same sampled value within one inference iteration</b>. This makes it easy for multiple components of your model to refer to the same logical random variable.\n",
    "    <li>Consequently, to define distinct random variables that correspond to different sampled values during a particular inference iteration, an effective practice is to add a dummy \"indexing\" parameter to the function. Distinct random variables can be referred to with different values for this index.\n",
    "    <li>Please see the documentation for more information about this decorator.\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bm.random_variable\n",
    "def z():\n",
    "    \"\"\"\n",
    "    An uninformative (flat) prior for z.\n",
    "    \"\"\"\n",
    "    # TODO(tingley): Replace with Flat once it's part of the framework.\n",
    "    return dist.Normal(0, 10000)\n",
    "\n",
    "@bm.random_variable\n",
    "def x():\n",
    "    \"\"\"\n",
    "    An uninformative (flat) prior for x.\n",
    "    \"\"\"\n",
    "    # TODO(tingley): Replace with Flat once it's part of the framework.\n",
    "    return dist.Normal(0, 10000)\n",
    "\n",
    "@bm.random_variable\n",
    "def neals_funnel_coin_flip():\n",
    "    \"\"\"\n",
    "    Flip a \"coin\", which is heads with probability equal to the probability\n",
    "    of drawing z and x from the true Neal's funnel posterior.\n",
    "    \"\"\"\n",
    "    return dist.Bernoulli(\n",
    "        (\n",
    "            dist.Normal(0.0, (z() / 2.0).exp()).log_prob(x()) +\n",
    "            dist.Normal(0.0, 3.0).log_prob(z())\n",
    "        ).exp()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Inference is the process of combining _model_ with _data_ to obtain _insights_, in the form of probability distributions over values of interest. Bean Machine offers a powerful and general inference framework to enable fitting arbitrary models to data.\n",
    "\n",
    "As discussed in the previous section, we'll pretend that we've observed heads when flipping a coin whose heads rate is weighted according to how likely the z and x values were to be drawn from the true Neal's funnel posterior. Let's set that up right now.\n",
    "\n",
    "Our inference algorithms expect observations in the form of a dictionary. This dictionary should consist of `@bm.random_variable` invocations as keys, and tensor data as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations={ neals_funnel_coin_flip(): tensor(1.0) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll run inference on the model and observations.\n",
    "\n",
    "Since this model is comprised entirely of differentiable random variables, we'll make use of the Newtonian Monte Carlo (NMC) inference method. NMC is a second-order method, which uses the Hessian to automatically scale the step size in each dimension. The hope is that this inference method will take the exponential growth rate of Neal's funnel into account, and explore the entire posterior surface, including the neck of the funnel. Check out the documentation for more information on NMC.\n",
    "\n",
    "Running inference consists of a few arguments:\n",
    "\n",
    "| Name | Usage\n",
    "| --- | ---\n",
    "| `queries` | A list of `@bm.random_variable` targets to fit posterior distributions for.\n",
    "| `observations` | The `Dict` of observations we built up, above.\n",
    "| `num_samples` | Number of samples to build up distributions for the values listed in `queries`.\n",
    "| `num_chains` | Number of separate inference runs to use. Multiple chains can verify inference ran correctly.\n",
    "\n",
    "Let's run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_site_nmc_samples = bm.SingleSiteNewtonianMonteCarlo().infer(\n",
    "    queries=[ z(), x() ],\n",
    "    observations=observations,\n",
    "    num_samples=1000,\n",
    "    num_chains=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "`samples` now contains our inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_marginal = single_site_nmc_samples[ z() ].flatten().detach()\n",
    "x_marginal = single_site_nmc_samples[ x() ].flatten().detach()\n",
    "\n",
    "print(\n",
    "    f\"z_marginal: {z_marginal}\\n\"\n",
    "    f\"x_marginal: {x_marginal}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our inferred posterior, along with the marginal distributions for $z$ and $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = mpl.gridspec.GridSpec(4, 4)\n",
    "\n",
    "plt.subplot(grid[1:, :3])\n",
    "plt.contour(xs, zs, density.log(), levels=range(-10, 0), zorder=0)\n",
    "plt.scatter(x_marginal, z_marginal, alpha=0.25)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.ylim(-15, 15)\n",
    "\n",
    "plt.subplot(grid[0, :3])\n",
    "plt.hist(x_marginal, bins=60, density=True, range=(-50, 50))\n",
    "plt.ylabel(\"density\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.gca().axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "plt.subplot(grid[1:, 3])\n",
    "zs_marginal = torch.linspace(-10, 10, 100)\n",
    "plt.hist(z_marginal, bins=30, density=True, range=(-15, 15), orientation=\"horizontal\")\n",
    "plt.plot(dist.Normal(0, 3).log_prob(zs_marginal).exp(), zs_marginal, color=\"black\")\n",
    "plt.xlabel(\"density\")\n",
    "plt.ylim(-15, 15)\n",
    "plt.gca().axes.get_yaxis().set_ticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples appear to match the correct posterior well! Inference also seems to have successfully fit the $z$ marginal, which is analytically known.\n",
    "\n",
    "Bean Machine provides a Diagnostics package that provides helpful statistics about the result of the inference algorithm. We can query this information as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Diagnostics(single_site_nmc_samples).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z$ and $x$ have means very close to zero, which is expected. $x$ as a much higher standard deviation than $z$, which is expected as well. The quantiles give useful insights into the spread of the two variables.\n",
    "\n",
    "The diagnostics output shows two diagnostic statistics: [$\\hat{R}$](https://projecteuclid.org/euclid.ss/1177011136) (`r_hat`) and [$N_\\text{eff}$](https://www.mcmchandbook.net/HandbookChapter1.pdf) (effective sample size, `n_eff`).\n",
    "\n",
    "  * $\\hat{R} \\in [1, \\infty)$ summarizes how effective inference was at converging on the correct posterior distribution for a particular random variable. It uses information from all chains run in order to assess whether inference had a good understanding of the distribution or not. Values very close to zero indicate that all chains discovered similar distributions for a particular random variable. We do not recommend using inference results where $\\hat{R} > 1.1$, as inference may not have converged. In that case, you may want to run inference for more samples.\n",
    "  * $N_\\text{eff} \\in [1, \\texttt{num}\\_\\texttt{samples}]$ summarizes how independent posterior samples are from one another. Although inference was run for `num_samples` iterations, it's possible that those samples were very similar to each other (due to the way inference is implemented), and may not each be representative of the full posterior space. Larger numbers are better here, and if your particular use case calls for a certain number of samples to be considered, you should ensure that $N_\\text{eff}$ is at least that large.\n",
    "  \n",
    "In this case, $\\hat{R}$ and $N_\\text{eff}$ seem to have acceptable values.\n",
    "\n",
    "Bean Machine can also plot diagnostical information to assess model fit. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Diagnostics(single_site_nmc_samples).plot(display=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagnostics output shows two diagnostic plots for individual random variables: trace plots and autocorrelation plots.\n",
    "\n",
    "  * Trace plots are simply a time series of values assigned to random variables over each iteration of inference. The concrete values assigned are usually problem-specific. However, it's important that these values are \"mixing\" well over time. This means that they don't tend to get stuck in one region for large periods of time, and that each of the chains ends up exploring the same space as the other chains throughout the course of inference.\n",
    "  * Autocorrelation plots measure how predictive the last several samples are of the current sample. Autocorrelation may vary between -1.0 (deterministically anticorrelated) and 1.0 (deterministically correlated). (We compute autocorrelation approximately, so it may sometimes exceed these bounds.) In an ideal world, the current sample is chosen independently of the previous samples: an autocorrelation of zero. This is not possible in practice, due to stochastic noise and the mechanics of how inference works.\n",
    "\n",
    "From the autocorrelation plots, we see the absolute mangitude of autocorrelation tends to be quite small. The trace plots are a little more suspicious, especially for $x$. Let's take a deeper look at the spike in chain 3. Here, if we look at the corresponding trace plot for $z$ at this time, we see that it is exploring large outlier values for $z$, around 6 or greater. We expect $x$ to have high variance when $z$ is large, so this is as expected.\n",
    "\n",
    "This concludes the main Neal's funnel tutorial! However, we'll also walk through the same model using Hamiltonian Monte Carlo inference to compare relative performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Single-site Hamiltonian Monte Carlo\n",
    "\n",
    "Hamiltonian Monte Carlo is a classic gradient-based inference method. HMC proceeds by taking a sequence of steps towards the gradient, but with some injected noise, before proposing a candidate sample. Bean Machine provides a single-site implementation of HMC that we can use to fit Neal's funnel. Check out our documentation for more information on HMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_site_hmc_samples = bm.SingleSiteHamiltonianMonteCarlo(\n",
    "    path_length=0.1,\n",
    "    step_size=0.01,\n",
    ").infer(\n",
    "    queries=[ z(), x() ],\n",
    "    observations=observations,\n",
    "    num_samples=1000,\n",
    "    num_chains=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_marginal = single_site_hmc_samples[ z() ].flatten().detach()\n",
    "x_marginal = single_site_hmc_samples[ x() ].flatten().detach()\n",
    "\n",
    "print(\n",
    "    f\"z_marginal: {z_marginal}\\n\"\n",
    "    f\"x_marginal: {x_marginal}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = mpl.gridspec.GridSpec(4, 4)\n",
    "\n",
    "plt.subplot(grid[1:, :3])\n",
    "plt.contour(xs, zs, density.log(), levels=range(-10, 0), zorder=0)\n",
    "plt.scatter(x_marginal, z_marginal, alpha=0.25)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.ylim(-15, 15)\n",
    "\n",
    "plt.subplot(grid[0, :3])\n",
    "plt.hist(x_marginal, bins=60, density=True, range=(-50, 50))\n",
    "plt.ylabel(\"density\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.gca().axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "plt.subplot(grid[1:, 3])\n",
    "zs_marginal = torch.linspace(-10, 10, 100)\n",
    "plt.hist(z_marginal, bins=60, density=True, range=(-15, 15), orientation=\"horizontal\")\n",
    "plt.plot(dist.Normal(0, 3).log_prob(zs_marginal).exp(), zs_marginal, color=\"black\")\n",
    "plt.xlabel(\"density\")\n",
    "plt.ylim(-15, 15)\n",
    "plt.gca().axes.get_yaxis().set_ticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, HMC isn't able to fully explore values for $z$, which prevents it from correctly recovering the posterior. We can confirm that HMC hasn't mixed well by examining the $\\hat{R}$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Diagnostics(single_site_hmc_samples).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the unhealthy trace and autocorrelation plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.Diagnostics(single_site_hmc_samples).plot(display=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Metropolis-Adjusted Langevin Algorithm\n",
    "\n",
    "The Metropolis-Adjusted Langevin Algorithm (MALA) is a special case of HMC, where only a single gradient step is taken before proposing a sample. Let's try it on Neal's funnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_site_mala_samples = bm.SingleSiteHamiltonianMonteCarlo(\n",
    "    path_length=0.5,\n",
    "    step_size=0.5,\n",
    ").infer(\n",
    "    queries=[ z(), x() ],\n",
    "    observations=observations,\n",
    "    num_samples=2000,\n",
    "    num_chains=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_marginal = single_site_mala_samples[ z() ].flatten().detach()\n",
    "x_marginal = single_site_mala_samples[ x() ].flatten().detach()\n",
    "\n",
    "print(\n",
    "    f\"z_marginal: {z_marginal}\\n\"\n",
    "    f\"x_marginal: {x_marginal}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = mpl.gridspec.GridSpec(4, 4)\n",
    "\n",
    "plt.subplot(grid[1:, :3])\n",
    "plt.contour(xs, zs, density.log(), levels=range(-10, 0), zorder=0)\n",
    "plt.scatter(x_marginal, z_marginal, alpha=0.25)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.ylim(-15, 15)\n",
    "\n",
    "plt.subplot(grid[0, :3])\n",
    "plt.hist(x_marginal, bins=60, density=True, range=(-50, 50))\n",
    "plt.ylabel(\"density\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.gca().axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "plt.subplot(grid[1:, 3])\n",
    "zs_marginal = torch.linspace(-10, 10, 100)\n",
    "plt.hist(z_marginal, bins=60, density=True, range=(-15, 15), orientation=\"horizontal\")\n",
    "plt.plot(dist.Normal(0, 3).log_prob(zs_marginal).exp(), zs_marginal, color=\"black\")\n",
    "plt.xlabel(\"density\")\n",
    "plt.ylim(-15, 15)\n",
    "plt.gca().axes.get_yaxis().set_ticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, MALA seems capable of fitting Neal's funnel better than HMC!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Global Hamiltonian Monte Carlo\n",
    "\n",
    "To implement global HMC, we would like to use block inference for $z$ and $x$. If our inference proposes a new value for $z$, it will also jointly propose a new value for $x$.\n",
    "\n",
    "We configure block inference using the method `add_sequential_proposer`, which takes a list of variables as an argument. Note that, unlike in typical Bean Machine syntax, we don't include arguments to variable objects when defining the block inference scheme. This is because the framework can compute the minimal _specific_ set of variables to update from the random variable families provided to this function. The next paragraph explains how that block selection is performed, and is quite technical! Feel free to skip it if desired.\n",
    "\n",
    "<div style=\"background: #daeaf3; border-left: 3px solid #2980b9; display: block; margin: 16px 0; padding: 12px;\">\n",
    "  Block inference selects specific subsets of random variables to update jointly. This is based upon which variables would typically be included in one another's Markov blanket during a single-site update. The Markov blanket of a random variable is a statistical concept defining which  other random variables may be directly affected when we propose a new value for the current random variable. It includes the current random variable's parents, children, and parents-of-children. Therefore, <code>z</code> and <code>x</code> <i>always</i> end up blocked together in this simple example. Our single-site inference typically runs inference individually for <code>z</code> and <code>x</code>, doing separate accept-reject procedures for each individual node of the probabilistic model. With block inference, before computing the accept-reject probability on <code>z</code>, the method will search the Markov blanket of <code>z</code> to find all variables that would be directly affected by an update to <code>z</code>. In this case, <code>x</code> will be blocked together with <code>z</code>, since that's what we configured with <code>add_sequential_proposer</code>. New proposals for <i>all</i> of these variables will be jointly accepted or rejected together. Read more about block inference in our documentation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_hmc_inference_strategy = bm.CompositionalInference({\n",
    "    z: SingleSiteHamiltonianMonteCarloProposer(path_length=0.1, step_size=0.01),\n",
    "    x: SingleSiteHamiltonianMonteCarloProposer(path_length=0.1, step_size=0.01),\n",
    "})\n",
    "global_hmc_inference_strategy.add_sequential_proposer([z, x])\n",
    "global_hmc_samples = global_hmc_inference_strategy.infer(\n",
    "    queries=[ z(), x() ],\n",
    "    observations=observations,\n",
    "    num_samples=1000,\n",
    "    num_chains=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_marginal = global_hmc_samples[ z() ].flatten().detach()\n",
    "x_marginal = global_hmc_samples[ x() ].flatten().detach()\n",
    "\n",
    "print(\n",
    "    f\"z_marginal: {z_marginal}\\n\"\n",
    "    f\"x_marginal: {x_marginal}\"\n",
    ")\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(4, 4)\n",
    "\n",
    "plt.subplot(grid[1:, :3])\n",
    "plt.contour(xs, zs, density.log(), levels=range(-10, 0), zorder=0)\n",
    "plt.scatter(x_marginal, z_marginal, alpha=0.25)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.ylim(-15, 15)\n",
    "\n",
    "plt.subplot(grid[0, :3])\n",
    "plt.hist(x_marginal, bins=60, density=True, range=(-50, 50))\n",
    "plt.ylabel(\"density\")\n",
    "plt.xlim(-50, 50)\n",
    "plt.gca().axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "plt.subplot(grid[1:, 3])\n",
    "zs_marginal = torch.linspace(-10, 10, 100)\n",
    "plt.hist(z_marginal, bins=60, density=True, range=(-15, 15), orientation=\"horizontal\")\n",
    "plt.plot(dist.Normal(0, 3).log_prob(zs_marginal).exp(), zs_marginal, color=\"black\")\n",
    "plt.xlabel(\"density\")\n",
    "plt.ylim(-15, 15)\n",
    "plt.gca().axes.get_yaxis().set_ticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields similar results to single-site HMC inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMGInference\n",
    "\n",
    "Bean Machine Graph (BMG) Inference is an experimental feature of the Bean Machine framework that aims to deliver higher performance for specialized models. The model in this tutorial is _almost_ but not quite in the subset of the language supported by BMGInference. In particular, `log_prob` is not yet supported. But we can rewrite our model to avoid it the need for `log_prob` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def normal_log_prob(mu, sigma, x):\n",
    "    z = (x - mu) / sigma\n",
    "    return -math.log(sigma) + (-1.0 / 2.0) * math.log(2.0 * math.pi) - (z ** 2.0 / 2.0)\n",
    "\n",
    "\n",
    "@bm.random_variable\n",
    "def neals_funnel_coin_flip_bmg():\n",
    "    \"\"\"\n",
    "    Flip a \"coin\", which is heads with probability equal to the probability\n",
    "    of drawing z and x from the true Neal's funnel posterior.\n",
    "    \"\"\"\n",
    "    return dist.Bernoulli(\n",
    "        (\n",
    "            normal_log_prob(0.0, 3.0, z())\n",
    "            + normal_log_prob(0.0, (z() / 2.0).exp(), x())\n",
    "        ).exp()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this definition in our model, and in particular, in our notion of observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations={ neals_funnel_coin_flip_bmg(): tensor(1.0) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing this tutorial, only Newtonian Monte Carlo (NMC) inference was supported by BMGInference. So, as a reference point, the following code reports the time it takes for our basic implementation of NMC to compute the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "single_site_nmc_samples = bm.SingleSiteNewtonianMonteCarlo().infer(\n",
    "    queries=[ z(), x() ],\n",
    "    observations=observations,\n",
    "    num_samples=1000,\n",
    "    num_chains=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our model using BMGInference, the only change needed is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from beanmachine.ppl.inference.bmg_inference import BMGInference\n",
    "\n",
    "single_site_bmg_samples = BMGInference().infer(\n",
    "    queries=[ z(), x() ],\n",
    "    observations=observations,\n",
    "    num_samples=1000,\n",
    "    num_chains=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wall time numbers will naturally vary on different platforms, but with with these parameters \n",
    "(model, observations, queries, sample size, and number of chains) speedup on the author's machine is about 7x. \n",
    "Generally speaking, larger speedups are expected with larger sample sizes. \n",
    "More information about BMGInference can be found on the website in \"Advanced\" section of the documentation.\n"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "273075370740593"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200811-093407",
   "data_retention_policy": "default",
   "description": "Run Nealâ€™s funnel in Bean Machine with single-site HMC and single-site NMC inferences.\n",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "deps": [
     "//beanmachine:bento_deps"
    ],
    "external_deps": []
   },
   "no_uii": true,
   "notebook_number": "273308",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "353966372283295",
   "tags": "python",
   "tasks": "",
   "title": "Neal funnel in Bean Machine with various inference"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
