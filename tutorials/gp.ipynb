{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression (with GPytorch)\n",
    "This tutorial assumes familiarity with the following:\n",
    "1. Bean Machine [modeling](#link) and [inference](#link)\n",
    "2. [Gaussian Processes](https://en.wikipedia.org/wiki/Gaussian_process)\n",
    "3. [GPyTorch](https://gpytorch.ai/)\n",
    "\n",
    "A Gaussian Process (GP) is a stochastic process commonly used in Bayesian non-parametrics, whose finite collection of random variables follow a multivariate Gaussian distribution.  GPs are fully defined by a mean and covariance function:\n",
    "\n",
    "$$f \\sim \\mathcal{GP}\\left(\\mu(x), \\mathbf{K}_f(x, x')\\right)$$\n",
    "\n",
    "where  $x, x' \\in\\mathbf{X}$ are two data points (e.g. train and test), $\\mu$ is the mean function (usually taken to be zero or constant), and $\\mathbf{K}_f$ is the kernel function, which computes a covariance given two datapoints and a distance metric.\n",
    "\n",
    "The aim is then to fit a posterior over *functions*.  GPs allow us to learn a distribution over functions given our observed data and predict unseen data with well-calibrated uncertainty, and is commonly used in Bayesian Optimization as a surrogate function to maximize an objective. For a thorough introduction to Gaussian processes, please see [1]\n",
    "\n",
    "With a PPL such as Bean Machine, we can be Bayesian about the parameters we care about, i.e. learn posterior distributions over these parameters rather than a point estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "import beanmachine\n",
    "import beanmachine.ppl as bm\n",
    "from beanmachine.ppl.inference.single_site_no_u_turn_sampler import SingleSiteNoUTurnSampler\n",
    "import beanmachine.ppl.experimental.gp as bgp\n",
    "from beanmachine.ppl.experimental.gp.models import SimpleGP\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.kernels import Kernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "smoke_test = ('SANDCASTLE_NEXUS' in os.environ or 'CI' in os.environ)\n",
    "\n",
    "print('pytorch version: ', torch.__version__)\n",
    "print('gpytorch version: ', gpytorch.__version__)\n",
    "# print('beanmachine version: ', beanmachine.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some simple cyclic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.linspace(0, 1, 11)\n",
    "y_train = torch.sin(x_train * (2 * math.pi)) + torch.randn(x_train.shape) * 0.2\n",
    "x_test = torch.linspace(0, 1, 51).unsqueeze(-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.scatter(x_train.numpy(), y_train.numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data has a periodic trend to it, we will use a Periodic Kernel:\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\Big(-\\frac{2}{\\ell}\\sin^2\\Big(\\pi\\frac{|x - x'|}{p}\\Big)\\Big)$$\n",
    "\n",
    "where $p$, $\\ell$, $\\sigma^2$ are the periodicity, lengthscale, and outputscale of the function respectively, the (hyper)parameters of the kernel we want to learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Estimation (with GPyTorch)\n",
    "GPytorch's exact inference algorithms allow you to compute maximum a posteriori (MAP) estimates of kernel parameters. Since a `SimpleGP` extends a GPytorch ExactGP model, you can use GPytorch to optimize the model. Let's try that, closely following the [GPytorch regression tutorial](https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(SimpleGP):\n",
    "    def __init__(self,\n",
    "                 x_train,\n",
    "                 y_train,\n",
    "                 mean,\n",
    "                 kernel,\n",
    "                 likelihood,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__(x_train, y_train, mean, kernel, likelihood)\n",
    "\n",
    "    def forward(self, data, batch_shape=()):\n",
    "        \"\"\"\n",
    "        Computes the GP prior given data. This method should always\n",
    "        return a `torch.distributions.MultivariateNormal`\n",
    "        \"\"\"\n",
    "        shape = data.shape[len(batch_shape)]\n",
    "        jitter = torch.eye(shape, shape) * 1e-5\n",
    "        for _ in range(len(batch_shape)):\n",
    "            jitter = jitter.unsqueeze(0)\n",
    "        if isinstance(self.mean, gpytorch.means.Mean): \n",
    "            # demo using gpytorch for MAP estimation\n",
    "            mean = self.mean(data)\n",
    "        else:\n",
    "            # use Bean Machine for learning posteriors\n",
    "            if self.training:\n",
    "                mean = self.mean(batch_shape).expand(data.shape[len(batch_shape):])\n",
    "            else:\n",
    "                mean = self.mean.expand(data.shape[:-1]) # overridden for evaluation\n",
    "        cov = self.kernel(data) + jitter\n",
    "        return MultivariateNormal(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(base_kernel=gpytorch.kernels.PeriodicKernel())\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "mean = gpytorch.means.ConstantMean()\n",
    "gp = Regression(x_train, y_train, mean, kernel, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(gp.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "gp.eval()  # this converts the BM model into a gpytorch model\n",
    "num_iters = 1 if smoke_test else 100\n",
    "\n",
    "for i in range(num_iters):\n",
    "    optimizer.zero_grad()\n",
    "    output = gp(x_train)\n",
    "    loss = -mll(output, y_train)\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (\n",
    "              i + 1, 100, loss.item(),\n",
    "             ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    observed_pred = likelihood(gp(x_test))\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(x_train.numpy(), y_train.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(x_test.squeeze().numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(x_test.squeeze().numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Our GP fits this simple function fairly well. However, we've only captured data uncertainty, not parameter uncertainty. It can often be the case that calibrating parameter uncertainty may lead to better predictive performance.  In the next section, we'll do just that using Bean Machine's NUTS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Bayesian Inference with Bean Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reuse the same model, but this time, use Bean Machine to learn posteriors over the parameters.  In `train` mode, `SimpleGP` is a simple wrapper around `gpytorch.models.ExactGP` that lifts learnable parameters to BM random variables.  Next, lets define our parameters $p$, $\\sigma^2$, and $\\ell$ in addition to mean and observation noise as random variables with the priors they are sampled from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bm.random_variable\n",
    "def outputscale():\n",
    "    return dist.Uniform(torch.tensor(1.), torch.tensor(2.))\n",
    "\n",
    "@bm.random_variable\n",
    "def lengthscale():\n",
    "    return dist.Uniform(torch.tensor([0.01]), torch.tensor([0.5]))\n",
    "\n",
    "@bm.random_variable\n",
    "def period_length():\n",
    "    return dist.Uniform(torch.tensor([0.05]), torch.tensor([2.5]))\n",
    "\n",
    "@bm.random_variable\n",
    "def noise():\n",
    "    return dist.Uniform(torch.tensor([0.05]), torch.tensor([0.3]))\n",
    "\n",
    "@bm.random_variable\n",
    "def mean(batch_shape=()):\n",
    "    batch_shape += (1,)\n",
    "    a = -1 * torch.ones(batch_shape)\n",
    "    b = torch.ones(batch_shape)\n",
    "    return dist.Uniform(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we'll create kernel and likelihood objects, this time passing our random variables as the hyperparameters. Note that the kernels are `beanmachine.kernels` instead of `gpytorch.kernels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = bgp.kernels.ScaleKernel(\n",
    "            base_kernel=bgp.kernels.PeriodicKernel(period_length_prior=period_length,\n",
    "                                                   lengthscale_prior=lengthscale),\n",
    "            outputscale_prior=outputscale\n",
    "        )\n",
    "likelihood = bgp.likelihoods.GaussianLikelihood(noise_prior=noise)\n",
    "\n",
    "gp = Regression(x_train, y_train, mean, kernel, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run inference as we would with any other Bean Machine model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1 if smoke_test else 100\n",
    "\n",
    "# bind the data to the forward call so it can be invoked in the likelihood\n",
    "gp_prior = partial(gp, x_train)\n",
    "obs = {gp.likelihood(gp_prior): y_train}\n",
    "\n",
    "nuts = SingleSiteNoUTurnSampler()\n",
    "samples = nuts.infer([mean(), lengthscale(), period_length(),\n",
    "                      outputscale(), noise()],\n",
    "                     obs,\n",
    "                     num_samples=num_samples,\n",
    "                     num_adaptive_samples=10,\n",
    "                     num_chains=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how our model fit. We will plot the samples of our posterior as well as the predictives generated from our GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beanmachine.ppl.diagnostics.diagnostics import Diagnostics\n",
    "Diagnostics(samples).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthscale_samples = samples.get_chain(0)[lengthscale()]\n",
    "outputscale_samples = samples.get_chain(0)[outputscale()]\n",
    "period_length_samples = samples.get_chain(0)[period_length()]\n",
    "mean_samples = samples.get_chain(0)[mean()]\n",
    "noise_samples = samples.get_chain(0)[noise()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not smoke_test:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.distplot(lengthscale_samples, label='lengthscale')\n",
    "    sns.distplot(outputscale_samples, label='outputscale')\n",
    "    sns.distplot(period_length_samples[:int(num_samples/2)], label='periodlength')\n",
    "    plt.legend()\n",
    "    plt.title(\"Posterior Empirical Distribution\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate predictions, we will convert our model to a Gpytorch model by running in `eval` mode. We load our posterior samples with a python dict, keyed on the parameter namespace and valued on the torch tensor of samples. Note the `unsqueeze`s to allow broadcasting of the data dimension to the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.eval() # converts to Gpytorch model in eval mode\n",
    "gp.bm_load_samples({'kernel.outputscale': outputscale_samples,\n",
    "               'kernel.base_kernel.lengthscale': lengthscale_samples.unsqueeze(-1),\n",
    "               'kernel.base_kernel.period_length': period_length_samples.unsqueeze(-1),\n",
    "               'likelihood.noise': noise_samples,\n",
    "               'mean': mean_samples,\n",
    "              })\n",
    "expanded_x_test = x_test.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "output = gp(expanded_x_test.detach(), batch_shape=(num_samples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we let's plot a few predictive samples from our GP. As you can see, we can draw different kernels, each of which paramaterizes a Multivariate Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not smoke_test:\n",
    "    with torch.no_grad():\n",
    "        f, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "        ax.plot(x_train.numpy(), y_train.numpy(), 'k*', zorder=10)\n",
    "        ax.plot(x_test.numpy(), output.mean.median(0)[0].detach().numpy(), 'b', linewidth=1.5)\n",
    "        for i in range(min(20, num_samples)):\n",
    "            ax.plot(x_test.numpy(), output.mean[i].detach().numpy(), 'gray', linewidth=0.3, alpha=0.8)\n",
    "        ax.legend(['Observed Data', 'Median', 'Sampled Means'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Rasmussen, Carl and Williams, Christopher. **Gaussian Processes for Machine Learning**. 2006."
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "2704931363113793"
  },
  "disseminate_notebook_info": {
   "bento_version": "20201213-210235",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "deps": [
     "//beanmachine:bento_deps"
    ],
    "external_deps": []
   },
   "no_uii": true,
   "notebook_number": "365217",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "142720057651189",
   "tags": "",
   "tasks": "",
   "title": "BM GP Tutorial"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
