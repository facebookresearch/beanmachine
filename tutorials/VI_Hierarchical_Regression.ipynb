{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport beanmachine.ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow Up To N1660040\n",
    "\n",
    "Run against https://github.com/feynmanliang/beanmachine/commit/70c76adcabf4a54f275a7d06a23ea257a9b1cf50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "N = 200\n",
    "X = dist.Normal(0, 1).expand([N, 1]).sample()\n",
    "true_beta_1 = 2.0\n",
    "true_beta_0 = 5.0\n",
    "true_epsilon = 1.0\n",
    "Y = dist.Normal(true_beta_1 * X + true_beta_0, true_epsilon).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8c7e8bdeb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Inference\n",
    "\n",
    "Convert all priors to uninformatives (`Flat`s) so that only likelihood factors are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beanmachine.ppl as bm\n",
    "from beanmachine.ppl.distributions import Flat, Unit, Delta\n",
    "\n",
    "class LinearRegressionFreq:\n",
    "    @bm.random_variable\n",
    "    def sigma_beta_1(self):\n",
    "        return Flat()\n",
    "\n",
    "    @bm.random_variable\n",
    "    def beta_1(self):\n",
    "        return Flat()\n",
    "\n",
    "    @bm.random_variable\n",
    "    def beta_0(self):\n",
    "        return Flat()\n",
    "\n",
    "    @bm.random_variable\n",
    "    def epsilon(self):\n",
    "        return Flat()\n",
    "\n",
    "    @bm.random_variable\n",
    "    def y(self, X):\n",
    "        return dist.Normal(self.beta_1() * X + self.beta_0(), torch.nn.functional.softplus(self.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform VI using `Delta`s to obtain point estimates maximizing ELBO $\\propto$ likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f847f1a6886048e9a04c79e6809e0ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "model = LinearRegressionFreq()\n",
    "\n",
    "@bm.param\n",
    "def phi():\n",
    "    return torch.tensor([0, 0, 1]).float()\n",
    "\n",
    "@bm.random_variable\n",
    "def q_beta_0():\n",
    "    return Delta(phi()[0])\n",
    "\n",
    "@bm.random_variable\n",
    "def q_beta_1():\n",
    "    return Delta(phi()[1])\n",
    "\n",
    "@bm.random_variable\n",
    "def q_eps():\n",
    "    return Delta(phi()[2])\n",
    "\n",
    "\n",
    "world_mle = bm.VariationalInfer(\n",
    "    {\n",
    "        model.beta_0(): q_beta_0(),\n",
    "        model.beta_1(): q_beta_1(),\n",
    "        model.epsilon(): q_eps(),\n",
    "    },\n",
    "    observations={model.y(X): Y},\n",
    "    optimizer=lambda params: optim.Adam(params, lr=1e-2)\n",
    ").infer(\n",
    "    num_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<function q_beta_0 at 0x7f8c7e827940>, tensor(4.4322, requires_grad=True)), (<function q_beta_1 at 0x7f8c7e827a60>, tensor(1.9348, requires_grad=True))]\n",
      "[(<function q_eps at 0x7f8c7e827b80>, tensor(1.9952, grad_fn=<SoftplusBackward0>))]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print([(rvid, world_mle.get_variable(rvid()).value) for rvid in (q_beta_0, q_beta_1,)])\n",
    "print([(rvid, F.softplus(world_mle.get_variable(rvid()).value)) for rvid in (q_eps,)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE estimates are close to $\\beta_0 = 5.0$, $\\beta_1 = 2.0$, $\\epsilon = 1.0$, which agrees with the theory on consistency of MLEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 2.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(true_beta_0, true_beta_1, true_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Inference\n",
    "\n",
    "Keep priors around so the world `log_prob` is proportional to the posterior. VI with `Delta`s effectively performs MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc70873ec7d448b9b4d5e0e414331c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearRegressionBayes:\n",
    "    @bm.random_variable\n",
    "    def sigma_beta_1(self):\n",
    "        return dist.Gamma(1, 1)\n",
    "\n",
    "    @bm.random_variable\n",
    "    def beta_1(self):\n",
    "        return dist.Normal(0, self.sigma_beta_1())\n",
    "\n",
    "    @bm.random_variable\n",
    "    def beta_0(self):\n",
    "        return dist.Normal(0,  10)\n",
    "\n",
    "    @bm.random_variable\n",
    "    def epsilon(self):\n",
    "        return dist.Gamma(1, 1)\n",
    "\n",
    "    @bm.random_variable\n",
    "    def y(self, X):\n",
    "        return dist.Independent(\n",
    "            dist.Normal(self.beta_1() * X + self.beta_0(), self.epsilon()),\n",
    "            0)\n",
    "\n",
    "\n",
    "model = LinearRegressionBayes()\n",
    "\n",
    "@bm.param\n",
    "def phi():\n",
    "    return torch.tensor([0, 0, 1]).float()\n",
    "\n",
    "@bm.random_variable\n",
    "def q_beta_0():\n",
    "    return Delta(phi()[0])\n",
    "\n",
    "@bm.random_variable\n",
    "def q_beta_1():\n",
    "    return Delta(phi()[1])\n",
    "\n",
    "@bm.random_variable\n",
    "def q_eps():\n",
    "    return Delta(phi()[2])\n",
    "\n",
    "\n",
    "world_map = bm.VariationalInfer(\n",
    "    {\n",
    "        model.beta_0(): q_beta_0(),\n",
    "        model.beta_1(): q_beta_1(),\n",
    "        model.epsilon(): q_eps(),\n",
    "    },\n",
    "    observations={model.y(X): Y},\n",
    "    optimizer=lambda params: optim.Adam(params, lr=1e-2),\n",
    ").infer(\n",
    "    num_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<function q_beta_0 at 0x7f8c7e7e4160>, tensor(3.8962, requires_grad=True)), (<function q_beta_1 at 0x7f8c7e7e4280>, tensor(-0.0038, requires_grad=True))]\n",
      "[(<function q_eps at 0x7f8c7e7e43a0>, tensor(2.6429, grad_fn=<SoftplusBackward0>))]\n"
     ]
    }
   ],
   "source": [
    "print([(rvid, world_map.get_variable(rvid()).value) for rvid in (q_beta_0, q_beta_1,)])\n",
    "print([(rvid, F.softplus(world_map.get_variable(rvid()).value)) for rvid in (q_eps,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 2.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(true_beta_0, true_beta_1, true_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting MAP estimates are ``shrunken'' towards the origin, agreeing with a shrinkage view of Bayesian priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVI with a Bayesian Linear Regression NNet\n",
    "\n",
    "Next, consider a variational model more complicated than `Delta`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets the guide $q(x)$ to be a simple linear neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d615d81ecc14e81a10f8614305b156a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearRegressionNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionNNet, self).__init__()\n",
    "\n",
    "    @bm.param\n",
    "    def mu(self):\n",
    "        return torch.zeros(1)\n",
    "\n",
    "    @bm.random_variable\n",
    "    def q_mu(self):\n",
    "        return Delta(self.mu())\n",
    "\n",
    "    @bm.param\n",
    "    def scale(self):\n",
    "        return torch.ones(1)\n",
    "\n",
    "    @bm.random_variable\n",
    "    def q_scale(self):\n",
    "        return Delta(self.scale())\n",
    "\n",
    "    @bm.random_variable\n",
    "    def mu_rv(self):\n",
    "        return Flat()  # MLE, no priors\n",
    "\n",
    "    @bm.random_variable\n",
    "    def scale_rv(self):\n",
    "        return Flat()  # MLE, no priors\n",
    "\n",
    "    @bm.random_variable\n",
    "    def forward(self, x):\n",
    "        return dist.Independent(dist.Normal(\n",
    "            loc=self.mu_rv() * X,\n",
    "            scale=F.softplus(self.scale_rv()),\n",
    "        ), 1)\n",
    "\n",
    "net = LinearRegressionNNet()\n",
    "\n",
    "world = bm.VariationalInfer(\n",
    "    {\n",
    "        net.mu_rv(): net.q_mu(),\n",
    "        net.scale_rv(): net.q_scale(),\n",
    "    },\n",
    "    {\n",
    "        net.forward(X): Y,\n",
    "    },\n",
    "    optimizer=lambda params: optim.Adam(params, lr=1e-1)\n",
    ").infer(\n",
    "    num_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.9932], requires_grad=True),\n",
       " tensor([5.1500], grad_fn=<SoftplusBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.get_param(net.mu()), F.softplus(world.get_param(net.scale()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (without intercept) least-squares estimate of $\\mu$ coincides with that obtained here by MLE; unsurprising as LSE is the MLE for Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9932]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pinverse(X) @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Delta` guide to a `Flat` prior seems redundant to `param`; can we make things more ergonomic?\n",
    "\n",
    "It turns out yes, if you (confusingly) use the same RV for the model and guide (the two cancel out in the density ratio) and furthermore observe on the same RV (so that the likelihood term is added to the `world.log_prob`). Then, optimization of variational `param`s literally corresponds to MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f580fcaa3a483f89249f03dc6782d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearRegressionNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionNNet, self).__init__()\n",
    "\n",
    "    @bm.param\n",
    "    def mu(self):\n",
    "        return torch.zeros(1)\n",
    "\n",
    "    @bm.param\n",
    "    def scale(self):\n",
    "        return torch.ones(1)\n",
    "    @bm.random_variable\n",
    "    def forward(self, x):\n",
    "        return dist.Independent(dist.Normal(\n",
    "            loc=self.mu() * X,\n",
    "            scale=F.softplus(self.scale()),\n",
    "        ), 1)\n",
    "\n",
    "net = LinearRegressionNNet()\n",
    "\n",
    "world = bm.VariationalInfer(\n",
    "    {\n",
    "        net.forward(X): net.forward(X),\n",
    "    },\n",
    "    {\n",
    "        net.forward(X): Y,\n",
    "    },\n",
    "    optimizer=lambda params: optim.Adam(params, lr=1e-1)\n",
    ").infer(\n",
    "    num_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.9932], requires_grad=True),\n",
       " tensor([5.1500], grad_fn=<SoftplusBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.get_param(net.mu()), F.softplus(world.get_param(net.scale()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the `Flat`s and `Delta`s from an earlier MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6a2f52a3c84599b3f0f656fd7cb8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import beanmachine.ppl as bm\n",
    "from beanmachine.ppl.distributions import Flat, Unit, Delta\n",
    "\n",
    "class LinearRegressionFreqSlim:\n",
    "    @bm.param\n",
    "    def beta_1(self):\n",
    "        return torch.zeros(1)\n",
    "\n",
    "    @bm.param\n",
    "    def beta_0(self):\n",
    "        return torch.zeros(1)\n",
    "\n",
    "    @bm.param\n",
    "    def epsilon(self):\n",
    "        return torch.ones(1)\n",
    "\n",
    "    @bm.random_variable\n",
    "    def y(self, X):\n",
    "        return dist.Normal(self.beta_1() * X + self.beta_0(), torch.nn.functional.softplus(self.epsilon()))\n",
    "\n",
    "model = LinearRegressionFreqSlim()\n",
    "\n",
    "world = bm.VariationalInfer(\n",
    "    {\n",
    "        model.y(X): model.y(X),\n",
    "    },\n",
    "    observations={model.y(X): Y},\n",
    "    optimizer=lambda params: optim.Adam(params, lr=1e-2)\n",
    ").infer(\n",
    "    num_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<bound method LinearRegressionFreqSlim.beta_0 of <__main__.LinearRegressionFreqSlim object at 0x7f8c7d514610>>, tensor([4.4322], requires_grad=True)), (<bound method LinearRegressionFreqSlim.beta_1 of <__main__.LinearRegressionFreqSlim object at 0x7f8c7d514610>>, tensor([1.9348], requires_grad=True))]\n",
      "[(<bound method LinearRegressionFreqSlim.epsilon of <__main__.LinearRegressionFreqSlim object at 0x7f8c7d514610>>, tensor([1.9952], grad_fn=<SoftplusBackward0>))]\n"
     ]
    }
   ],
   "source": [
    "print([(rvid, world.get_param(rvid())) for rvid in (model.beta_0, model.beta_1,)])\n",
    "print([(rvid, F.softplus(world.get_param(rvid()))) for rvid in (model.epsilon,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 2.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(true_beta_0, true_beta_1, true_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d13082b4bd245bb7b08aea3b25cb3b8185404358d859b3f501f7ea456008ea8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
